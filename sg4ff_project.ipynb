{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8e2e229738f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0msvm_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hinge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0msvm_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    924\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Got rid of the model 0.0 as part of the project overhaul but I got the data for the report from my checkpoint\n",
    "#Siddharth Ghatti(sg4ff)\n",
    "#CS 4501 Project\n",
    "import urllib\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import skimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix # optional\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def get_data():\n",
    "    full_data = {}\n",
    "    data_list = []\n",
    "    diagnosis = []\n",
    "    file = open('train-balanced-sarcasm.csv','r')\n",
    "    data_key = file.readline()\n",
    "    labels=[]\n",
    "    data = []\n",
    "    data_key = data_key.rstrip().split(',')\n",
    "    for line in file:\n",
    "        current_line = line.rstrip().split(',')\n",
    "        labels.append(current_line[0])\n",
    "        data.append(current_line[1])\n",
    "    return labels,data\n",
    "    \n",
    "        \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')      \n",
    "        \n",
    "        \n",
    "   \n",
    "        \n",
    "labels,data = get_data()\n",
    "count_vect = CountVectorizer()\n",
    "data =  count_vect.fit_transform(data)\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,labels, test_size=0.2, train_size=0.8, random_state=42)\n",
    "\n",
    "#Model 1.0 SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "svm_clf = LinearSVC(C=1, loss=\"hinge\", random_state=42)\n",
    "svm_clf.fit(X_train,y_train)\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "print(\"This is the data output for the Model 1.0 SVM \")\n",
    "print(\"This is the precision score \"+str(precision_score(y_test, y_pred, average='macro')))\n",
    "print(\"This is the recall score \"+str(recall_score(y_test, y_pred, average='macro')))\n",
    "#Precision and recall are the same when size is the same\n",
    "print(\"This is the accuracy \"+str(accuracy_score(y_test,y_pred)))\n",
    "print(\"This is the f1_score \"+str(f1_score(y_test,y_pred,average='macro')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the data output for the Model 2.0 Random Forest with only the comments\n",
      "This is the precision score 0.6676993343992397\n",
      "This is the recall score 0.660697284357258\n",
      "This is the accuracy 0.6607144623725058\n",
      "This is the f1_score 0.6571295477571726\n"
     ]
    }
   ],
   "source": [
    "#Model 1.0 Random Forest\n",
    "\n",
    "def get_data():\n",
    "    full_data = {}\n",
    "    data_list = []\n",
    "    diagnosis = []\n",
    "    file = open('train-balanced-sarcasm.csv','r')\n",
    "    data_key = file.readline()\n",
    "    labels=[]\n",
    "    data = []\n",
    "    data_key = data_key.rstrip().split(',')\n",
    "    for line in file:\n",
    "        current_line = line.rstrip().split(',')\n",
    "        labels.append(int(current_line[0]))\n",
    "        data.append(current_line[1])\n",
    "    return labels,data\n",
    "    \n",
    "        \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')      \n",
    "        \n",
    "        \n",
    "   \n",
    "        \n",
    "labels,data = get_data()\n",
    "count_vect = CountVectorizer()\n",
    "data =  count_vect.fit_transform(data)\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,labels, test_size=0.2, train_size=0.8, random_state=42)\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=256, max_depth=25,\n",
    "                             random_state=0)\n",
    "values = [20,64,128,256]\n",
    "depth = [4,8,12,25]\n",
    "parameters = {'n_estimators':values,'max_depth':depth}\n",
    "clf_f1 = GridSearchCV(random_forest_clf, parameters, cv=5,scoring='f1')\n",
    "clf_f1.fit(X_test, y_test)\n",
    "clf_f1.cv_results_\n",
    "F1_best = clf_f1.best_params_\n",
    "print(\"This is the best for F1 \"+str(F1_best))\n",
    "\n",
    "clf_precision = GridSearchCV(random_forest_clf, parameters, cv=5,scoring='precision')\n",
    "clf_precision.fit(X_test, y_test)\n",
    "clf_precision.cv_results_\n",
    "Precision_best= clf_precision.best_params_\n",
    "print(\"This is the best for Precision :\"+str(Precision_best))\n",
    "\n",
    "clf_accuracy = GridSearchCV(random_forest_clf, parameters, cv=5,scoring='accuracy')\n",
    "clf_accuracy.fit(X_test, y_test)\n",
    "clf_accuracy.cv_results_\n",
    "Accuracy_best = clf_accuracy.best_params_\n",
    "print(\"This is the best for accuracy :\"+str(Accuracy_best))\n",
    "\n",
    "clf_recall = GridSearchCV(random_forest_clf, parameters, cv=5,scoring='recall')\n",
    "clf_recall.fit(X_test, y_test)\n",
    "clf_recall.cv_results_\n",
    "Recall_best = clf_recall.best_params_\n",
    "print(\"This is the best for Recall :\"+str(Recall_best))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,labels, test_size=0.2, train_size=0.8, random_state=42)\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=256, max_depth=25,\n",
    "                             random_state=0)\n",
    "random_forest_clf.fit(X_train,y_train)\n",
    "y_pred_random_forest = random_forest_clf.predict(X_test)\n",
    "print(\"This is the data output for the Model 1.0 Random Forest\")\n",
    "print(\"This is the precision score \"+str(precision_score(y_test, y_pred_random_forest, average='macro')))\n",
    "print(\"This is the recall score \"+str(recall_score(y_test, y_pred_random_forest, average='macro')))\n",
    "#Precision and recall are the same when size is the same\n",
    "print(\"This is the accuracy \"+str(accuracy_score(y_test,y_pred_random_forest)))\n",
    "print(\"This is the f1_score \"+str(f1_score(y_test,y_pred_random_forest,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the best for F1 {'C': 1}\n",
      "This is the best for Precision :{'C': 0.01}\n",
      "This is the best for accuracy :{'C': 1}\n",
      "This is the best for Recall :{'C': 100}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#SVM model using TDIF instead of count vectorizer (Choosen to be the one that will be optomized further)\n",
    "import urllib\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import skimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix # optional\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def get_data():\n",
    "    full_data = {}\n",
    "    data_list = []\n",
    "    diagnosis = []\n",
    "    file = open('train-balanced-sarcasm.csv','r')\n",
    "    data_key = file.readline()\n",
    "    labels=[]\n",
    "    data = []\n",
    "    data_key = data_key.rstrip().split(',')\n",
    "    for line in file:\n",
    "        current_line = line.rstrip().split(',')\n",
    "        labels.append(int(current_line[0]))\n",
    "        data.append(current_line[1])\n",
    "    return labels,data\n",
    "    \n",
    "        \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')      \n",
    "        \n",
    "        \n",
    "#Model 1.5 SVM \n",
    "        \n",
    "labels,data = get_data()\n",
    "vectorizer = TfidfVectorizer()\n",
    "data = vectorizer.fit_transform(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,labels, test_size=0.2, train_size=0.8, random_state=42)\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "C= [0.01,0.1,1,100,500]\n",
    "svm_clf = LinearSVC(C=1, loss=\"hinge\", random_state=42)\n",
    "parameters = {'C':C}\n",
    "clf_f1 = GridSearchCV(svm_clf, parameters, cv=5,scoring='f1')\n",
    "clf_f1.fit(X_test, y_test)\n",
    "clf_f1.cv_results_\n",
    "F1_best = clf_f1.best_params_\n",
    "print(\"This is the best for F1 \"+str(F1_best))\n",
    "\n",
    "clf_precision = GridSearchCV(svm_clf, parameters, cv=5,scoring='precision')\n",
    "clf_precision.fit(X_test, y_test)\n",
    "clf_precision.cv_results_\n",
    "Precision_best= clf_precision.best_params_\n",
    "print(\"This is the best for Precision :\"+str(Precision_best))\n",
    "\n",
    "clf_accuracy = GridSearchCV(svm_clf, parameters, cv=5,scoring='accuracy')\n",
    "clf_accuracy.fit(X_test, y_test)\n",
    "clf_accuracy.cv_results_\n",
    "Accuracy_best = clf_accuracy.best_params_\n",
    "print(\"This is the best for accuracy :\"+str(Accuracy_best))\n",
    "\n",
    "clf_recall = GridSearchCV(svm_clf, parameters, cv=5,scoring='recall')\n",
    "clf_recall.fit(X_test, y_test)\n",
    "clf_recall.cv_results_\n",
    "Recall_best = clf_recall.best_params_\n",
    "print(\"This is the best for Recall :\"+str(Recall_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the data output for the Model 1.5 SVM with TDIF vectorizer\n",
      "This is the precision score 0.6775347120176618\n",
      "This is the recall score 0.6756985453953046\n",
      "This is the accuracy 0.6757070921915653\n",
      "This is the f1_score 0.6748636200273337\n",
      "[22442 27579 33909 16260 90920]\n",
      "but\n",
      "clearly\n",
      "dare\n",
      "because\n",
      "obviously\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "svm_clf.fit(X_train,y_train)\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "print(\"This is the data output for the Model 1.5 SVM with TDIF vectorizer\")\n",
    "print(\"This is the precision score \"+str(precision_score(y_test, y_pred, average='macro')))\n",
    "print(\"This is the recall score \"+str(recall_score(y_test, y_pred, average='macro')))\n",
    "    #Precision and recall are the same when size is the same\n",
    "print(\"This is the accuracy \"+str(accuracy_score(y_test,y_pred)))\n",
    "print(\"This is the f1_score \"+str(f1_score(y_test,y_pred,average='macro')))\n",
    "import numpy\n",
    "ind = np.argpartition(svm_clf.coef_[0], -5)[-5:]\n",
    "print(ind)\n",
    "for element in ind:\n",
    "    print(vectorizer.get_feature_names()[element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor Flow Model (Model 2.0 )\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) #reduce annoying warning messages\n",
    "from functools import partial\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_data():\n",
    "    full_data = {}\n",
    "    data_list = []\n",
    "    diagnosis = []\n",
    "    file = open('train-balanced-sarcasm.csv','r')\n",
    "    data_key = file.readline()\n",
    "    labels=[]\n",
    "    data = []\n",
    "    data_key = data_key.rstrip().split(',')\n",
    "    for line in file:\n",
    "        current_line = line.rstrip().split(',')\n",
    "        labels.append(current_line[0])\n",
    "        data.append(current_line[1])\n",
    "    return labels,data\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "labels,data = get_data()\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "data = tokenizer.texts_to_sequences(data)\n",
    "data = pad_sequences(data, maxlen = 200)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,labels, test_size=0.2, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "808660/808660 [==============================] - 111s 137us/sample - loss: 0.6959 - acc: 0.5311\n",
      "Epoch 2/20\n",
      "808660/808660 [==============================] - 101s 125us/sample - loss: 0.6883 - acc: 0.5353\n",
      "Epoch 3/20\n",
      "808660/808660 [==============================] - 100s 124us/sample - loss: 0.6873 - acc: 0.5375\n",
      "Epoch 4/20\n",
      "808660/808660 [==============================] - 105s 130us/sample - loss: 0.6865 - acc: 0.5389\n",
      "Epoch 5/20\n",
      "808660/808660 [==============================] - 101s 125us/sample - loss: 0.6861 - acc: 0.5404\n",
      "Epoch 6/20\n",
      "808660/808660 [==============================] - 103s 127us/sample - loss: 0.6862 - acc: 0.5398\n",
      "Epoch 7/20\n",
      "808660/808660 [==============================] - 117s 145us/sample - loss: 0.6859 - acc: 0.5407\n",
      "Epoch 8/20\n",
      "808660/808660 [==============================] - 118s 145us/sample - loss: 0.6857 - acc: 0.5416\n",
      "Epoch 9/20\n",
      "808660/808660 [==============================] - 122s 151us/sample - loss: 0.6854 - acc: 0.5426\n",
      "Epoch 10/20\n",
      "808660/808660 [==============================] - 118s 146us/sample - loss: 0.6856 - acc: 0.5422\n",
      "Epoch 11/20\n",
      "808660/808660 [==============================] - 127s 157us/sample - loss: 0.6854 - acc: 0.5418\n",
      "Epoch 12/20\n",
      "808660/808660 [==============================] - 126s 156us/sample - loss: 0.6854 - acc: 0.5423\n",
      "Epoch 13/20\n",
      "808660/808660 [==============================] - 115s 142us/sample - loss: 0.6853 - acc: 0.5427\n",
      "Epoch 14/20\n",
      "808660/808660 [==============================] - 129s 160us/sample - loss: 0.6852 - acc: 0.5428\n",
      "Epoch 15/20\n",
      "808660/808660 [==============================] - 123s 152us/sample - loss: 0.6853 - acc: 0.5436\n",
      "Epoch 16/20\n",
      "808660/808660 [==============================] - 111s 137us/sample - loss: 0.6850 - acc: 0.5436\n",
      "Epoch 17/20\n",
      "808660/808660 [==============================] - 120s 148us/sample - loss: 0.6848 - acc: 0.5441\n",
      "Epoch 18/20\n",
      "808660/808660 [==============================] - 123s 152us/sample - loss: 0.6850 - acc: 0.5444\n",
      "Epoch 19/20\n",
      "808660/808660 [==============================] - 142s 176us/sample - loss: 0.6849 - acc: 0.5447\n",
      "Epoch 20/20\n",
      "808660/808660 [==============================] - 132s 163us/sample - loss: 0.6850 - acc: 0.5439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x131ebd400>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Conv2D,MaxPooling2D,Flatten,Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "model = Sequential()\n",
    "#input\n",
    "model.add(Dense(500, input_dim=200, activation='elu',kernel_initializer='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "#Hidden\n",
    "model.add(Dense(250, activation='elu',kernel_initializer='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100,activation='elu',kernel_initializer='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50,activation='elu',kernel_initializer='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "#Output\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, np.array(y_train),\n",
    "          epochs=20,\n",
    "          batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202166/202166 [==============================] - 9s 46us/sample - loss: 0.6940 - acc: 0.5001\n"
     ]
    }
   ],
   "source": [
    "y_test_new = []\n",
    "for element in y_test:\n",
    "    y_test_new.append(int(element))\n",
    "    \n",
    "score = model.evaluate(X_test, y_test_new, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source for Keras help : https://www.tensorflow.org/tutorials/keras/basic_text_classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
